{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norman-christopher",
   "metadata": {},
   "source": [
    "# NLP basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-internet",
   "metadata": {},
   "source": [
    "NLP modeling is no much difference from other ML models. The major difference is you first need to convert the text into vectors, and build model to make predictions. The prediction results could be a word, a sentence, a number, or a classification label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-porcelain",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font>  What is embedding? How to do embedding in NLP? List 5 ways to do embedding and explain how they are trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-shaft",
   "metadata": {},
   "source": [
    "Embed one-hot vectors into lower-dimens space. Address curse of dimensionality. \n",
    "\n",
    "Embedding is to convert some data into vectors. \n",
    "Count Vectorizer, \n",
    "TFIDF, \n",
    "word2vec, \n",
    "GloVe, factorizes a global co-occurrence matrix derived from th entire corpus. Low dimensional representations are obtained by solving a least-squares problem to 'recover' the co-occurence matrix. \n",
    "words*context^T = (words*Features)*(Features*Context). Similar to Latent Factor Vectores. \n",
    "\n",
    "fastText, represents each word as an n-gram of characters, alows the embeddings to understand suffixes and prefixes. Incorporate subword information into word embedding. \n",
    "Allows sharing subword representations across words, since words are represented by the aggregation of their n-grams. \n",
    "\n",
    "Network embedding: Embedding nodes in information networks. \n",
    "   LINE, DeepWalk, Metapath2Vec ...\n",
    "   \n",
    "wordPieces. \n",
    "The essence of embedding is the word \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-seeking",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Explain TFIDF. What are TF and IDF? What parameters can be tuned in TFIDF model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-lebanon",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Use your vectorizer to vecterize 'Jerry is a mouse. Tom is a cat.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-moses",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font>  How to embed a sentence? How to embed a paragraph? How to embed a product sold on Amazon? How to embed a customer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-sight",
   "metadata": {},
   "source": [
    "# NLP downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-sculpture",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> List all tasks you can find that NLP can solve now. Refer to GLUE, superGLUE, BLU, SQuAD, SWAG, CoNLL-2003, MultiNLI etc for reference. Given one application for each downstream task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-apparatus",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Given a word, what tasks can you perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer, stemmization, word to vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-testimony",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Given a sentence, what tasks can you perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment analysis, aspect sentiment analysis, NER, POS tagging, SRL, chunking, Masked Language Modeling, translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-divide",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Given a sentence pair, what tasks can you perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next sentence modeling, QA, sentence similarity, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-response",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Given a paragraph, what tasks can you perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-sodium",
   "metadata": {},
   "source": [
    "Text summarization, QA, Natural Language Inference. Completion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-england",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Given a big dictionary like wikipedia, what tasks can you perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre-training a big language model. Build knowledge graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-phrase",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-gateway",
   "metadata": {},
   "source": [
    "\n",
    "Paper: Attention is all your need https://arxiv.org/abs/1706.03762\n",
    "Youtube explanation: https://www.youtube.com/watch?v=nzqlFIcCSWQ&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-stockholm",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is a transformer architecture? Is transformer a CNN? Is transformer a RNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-system",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font>  What part of transformer is similar to the convolutional layer of CNN, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-dealing",
   "metadata": {},
   "source": [
    "Multi-head attention. Multi-channel input, multi-chanel output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-equipment",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What makes transformer so special and efficient compared to CNN and RNN is self-attention. Attention is all you need! Explain self attention. Why it is a good structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-blood",
   "metadata": {},
   "source": [
    "Self-Attention = kqv. It is highly paralizable, and the model can see the whole sentence at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-newfoundland",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is the advantage of transformer architecture compared to RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-palestinian",
   "metadata": {},
   "source": [
    "Can be parallelized, fast to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-simon",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Explain residual-connect module. What is the function of res-connect module in transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-florida",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What are the difference between the architectures of encoder and decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-order",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Write code to calculate one-head self-attention score given k, q, v. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-attack",
   "metadata": {},
   "source": [
    "# Bert Architecture\n",
    "Paper: https://arxiv.org/abs/1810.04805\n",
    "Youtube Video: https://www.youtube.com/watch?v=ULD3uIb2MHQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-somewhere",
   "metadata": {},
   "source": [
    "<font color='red'>Assignment:</font> Learn the architecture of BERT following the youtube video provided at least once, and answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-palace",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Give the full name of BERT, explain each word in the full name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-halifax",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is the relationship between BERT and transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-traffic",
   "metadata": {},
   "source": [
    "Both developed in google, BERT only uses encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-image",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What part/ideas of BERT are from ELMO and GPT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-athletics",
   "metadata": {},
   "source": [
    "ELMO uses RNN but is bi-directional. GPT uses transformer architecuture but uni-directional. BERT combines the bi-directional and transformer architecture. But BERT uses encoder but GPT uses decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-energy",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Explain why BERT needs positional embedding? How is positional embedding implemented (text OK, code preferred)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-marker",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> How is BERT trained? What is Masked Language Modeling (MLM) and Next sentence prediction (NSP)? Does it needs human labeling? What is this method of training sample construction called?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-advantage",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What hyper-parameters do you need to adjust for fine-tuning a BERT model? What are the typical ranges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-bargain",
   "metadata": {},
   "source": [
    "Batch size: 16, 32, 64, 128\n",
    "\n",
    "Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "\n",
    "Number of epochs: 2,3,4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-jones",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is a token? What is tokenizer? Is one token a word? What type of tokenizer does BERT adopt? How many tokens are there? Why the author of BERT used this kind of tokenizer instead of a big dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-manitoba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-kingston",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Distinguish the following methods: active learning, semi-supervised learning, self-supervised learning, weak supervised learning, contrastive learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-chuck",
   "metadata": {},
   "source": [
    "# Bert Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-conviction",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> Explain what is transfer learning, what is fine-tuning. Why we fine-tune the model instead of training from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-wrist",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What parameters are tuned when training a bert based model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-robinson",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> What is model distillation? Why we need model distillation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-constant",
   "metadata": {},
   "source": [
    "<font color='red'>Question:</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-traveler",
   "metadata": {},
   "source": [
    "# Hands-on Project: Bert based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-anatomy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
